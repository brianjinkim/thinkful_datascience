{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Utility functions\n",
    "def lemma_tokens(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading files\n",
    "filenames = ['against_the_gods', 'battle_through_the_heavens', 'desolate_era', 'emperors_domination', 'martial_god_asura', 'martial_world', 'overgeared', 'praise_the_orc', 'sovereign_of_the_three_realms', 'wu_dong_qian_kun']\n",
    "raw_files = []\n",
    "\n",
    "for filename in filenames:\n",
    "    with open('../dataset/' + filename + '.txt', encoding='utf-8') as myfile:\n",
    "        raw_files.append(myfile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_lemma = []\n",
    "totalvocab_tokenized = []\n",
    "for doc in raw_files:\n",
    "    allwords_lemma = lemma_tokens(doc) #for each item in doc, tokenize\n",
    "    totalvocab_lemma.extend(allwords_lemma) #extend the list to one flat array\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(doc)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 302940 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_lemma)\n",
    "print ('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>﻿yun</th>\n",
       "      <td>﻿yun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>che</th>\n",
       "      <td>che</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conscious</th>\n",
       "      <td>consciousness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradual</th>\n",
       "      <td>gradually</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   words\n",
       "﻿yun                ﻿yun\n",
       "che                  che\n",
       "'s                    's\n",
       "conscious  consciousness\n",
       "gradual        gradually"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 11356)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.75, # drop words that occur in more than 3/4 of the sentence\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #don't convert everything to lower case\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True, #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                             tokenizer=lemma_tokens,\n",
    "                             ngram_range=(1,3)                             \n",
    "                            )\n",
    "\n",
    "# Apply the vectorizer\n",
    "tfidf_matrix = vectorizer.fit_transform(raw_files)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth, SpectralClustering\n",
    "\n",
    "num_clusters = 6\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels = { 'title': filenames, 'text': raw_files, 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(novels, index = [clusters] , columns = ['title', 'text', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    3\n",
       "2    2\n",
       "1    2\n",
       "0    2\n",
       "4    1\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "----------------------------------------\n",
      "Cluster 0 words: quest, warrior, users, blacksmith, greatsword, wolves, game, granting, lord, axe\n",
      "Cluster 0 titles: overgeared, praise_the_orc\n",
      "\n",
      "Cluster 1 words: xiao, yan, s, xia, er, clan, t, qi, xiao, cloud\n",
      "Cluster 1 titles: against_the_gods, battle_through_the_heavens\n",
      "\n",
      "Cluster 2 words: lin, ming, dong, xiao, wang, tan, yan, seven, lin, yuan\n",
      "Cluster 2 titles: martial_world, wu_dong_qian_kun\n",
      "\n",
      "Cluster 3 words: feng, zheng, eastern, yue, qi, pills, grass, alliance, recipe, healing\n",
      "Cluster 3 titles: martial_god_asura, sovereign_of_the_three_realms\n",
      "\n",
      "Cluster 4 words: ji, snow, xiantian, clan, lord, cui, immortal, looked, diagram, sword\n",
      "Cluster 4 titles: desolate_era\n",
      "\n",
      "Cluster 5 words: li, sects, protector, disciple, saint, emperor, immortal, du, ancient, incense\n",
      "Cluster 5 titles: emperors_domination"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print('-'*40)\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    if i != 0:\n",
    "        print('\\n')\n",
    "    print(\"Cluster %d words: \" % i, end='')\n",
    "    \n",
    "    for j, ind in enumerate(order_centroids[i, :10]): #replace 6 with n words per cluster\n",
    "        if (j == 0):\n",
    "            print('%s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end='')\n",
    "        else:\n",
    "            print(', %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end='')\n",
    "    print()\n",
    "    \n",
    "    print(\"Cluster %d titles: \" % i, end='')\n",
    "    for j, title in enumerate(frame.loc[i]['title'].values.tolist()):\n",
    "        if (j == 0):\n",
    "            print('%s' % title, end='')\n",
    "        else:\n",
    "            print(', %s' % title, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(tfidf_matrix, test_size =0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters: 1\n"
     ]
    }
   ],
   "source": [
    "bandwidth = estimate_bandwidth(X_train.toarray())\n",
    "ms = MeanShift()\n",
    "ms.fit(tfidf_matrix.toarray())\n",
    "clusters = ms.labels_.tolist()\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "print(\"Number of estimated clusters: {}\".format(n_clusters_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters=5\n",
    "\n",
    "# Declare and fit the model.\n",
    "sc = SpectralClustering(n_clusters=n_clusters)\n",
    "sc.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels = { 'title': filenames, 'text': raw_files, 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(novels, index = [clusters] , columns = ['title', 'text', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>﻿yun</th>\n",
       "      <td>﻿yun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>che</th>\n",
       "      <td>che</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conscious</th>\n",
       "      <td>consciousness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradual</th>\n",
       "      <td>gradually</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awaken</th>\n",
       "      <td>awakened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   words\n",
       "﻿yun                ﻿yun\n",
       "che                  che\n",
       "'s                    's\n",
       "conscious  consciousness\n",
       "gradual        gradually\n",
       "awaken          awakened\n",
       "what                what\n",
       "'s                    's\n",
       "go                 going\n",
       "on                    on"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "----------------------------------------\n",
      "Cluster 0 words: 'a, 'd, 'm, 'd, 'i, 'm, 'm, 'm, 'm, 'm\n",
      "Cluster 0 titles: overgeared, praise_the_orc\n",
      "\n",
      "Cluster 1 words: 'd, 'a, 'm, 'm, 'm, 'd, 'm, 'i, 'm, 'm\n",
      "Cluster 1 titles: against_the_gods, battle_through_the_heavens\n",
      "\n",
      "Cluster 2 words: 'd, 'm, 'm, 'i, 'm, 'a, 'd, 'm, 'm, 'm\n",
      "Cluster 2 titles: martial_world, wu_dong_qian_kun\n",
      "\n",
      "Cluster 3 words: 'i, 'm, 'm, 'd, 'a, 'm, 'm, 'm, 'd, 'm\n",
      "Cluster 3 titles: martial_god_asura, sovereign_of_the_three_realms\n",
      "\n",
      "Cluster 4 words: 'm, 'm, 'i, 'd, 'm, 'm, 'a, 'm, 'd, 'm\n",
      "Cluster 4 titles: desolate_era\n",
      "\n",
      "Cluster 5 words: 'm, 'm, 'd, 'm, 'a, 'i, 'd, 'm, 'm, 'm\n",
      "Cluster 5 titles: emperors_domination"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print('-'*40)\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = sc.affinity_matrix_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    if i != 0:\n",
    "        print('\\n')\n",
    "    print(\"Cluster %d words: \" % i, end='')\n",
    "    \n",
    "    for j, ind in enumerate(order_centroids[i, :10]): #replace 6 with n words per cluster\n",
    "        if (j == 0):\n",
    "            print('%s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end='')\n",
    "        else:\n",
    "            print(', %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end='')\n",
    "    print()\n",
    "    \n",
    "    print(\"Cluster %d titles: \" % i, end='')\n",
    "    for j, title in enumerate(frame.loc[i]['title'].values.tolist()):\n",
    "        if (j == 0):\n",
    "            print('%s' % title, end='')\n",
    "        else:\n",
    "            print(', %s' % title, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 9, 2, 3, 5, 8, 6, 7, 4],\n",
       "       [1, 0, 9, 8, 5, 2, 6, 3, 7, 4],\n",
       "       [2, 7, 8, 3, 6, 0, 1, 5, 4, 9],\n",
       "       [3, 8, 4, 2, 0, 7, 5, 6, 1, 9],\n",
       "       [4, 8, 3, 2, 7, 6, 0, 5, 1, 9],\n",
       "       [5, 9, 1, 8, 0, 3, 2, 6, 7, 4],\n",
       "       [6, 7, 8, 2, 5, 1, 3, 0, 4, 9],\n",
       "       [7, 6, 2, 8, 3, 5, 1, 4, 0, 9],\n",
       "       [8, 4, 1, 3, 7, 2, 6, 5, 0, 9],\n",
       "       [9, 5, 0, 1, 8, 2, 7, 6, 4, 3]], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_centroids = sc.affinity_matrix_.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words: \" % i, end='')\n",
    "    for j, ind in enumerate(order_centroids[i, :10]): #replace 6 with n words per cluster\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 9, 2, 3, 5, 8, 6, 7, 4], dtype=int64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_centroids[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'a\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.53173923, 0.14831525, 0.14583543, 0.13864201,\n",
       "        0.14442935, 0.14075505, 0.13886153, 0.14254869, 0.16894208],\n",
       "       [0.53173923, 1.        , 0.14461183, 0.14005377, 0.13739586,\n",
       "        0.15046942, 0.14217691, 0.13986649, 0.15949888, 0.16752942],\n",
       "       [0.14831525, 0.14461183, 1.        , 0.1534091 , 0.14080902,\n",
       "        0.14244372, 0.14985849, 0.18817661, 0.15472162, 0.13948326],\n",
       "       [0.14583543, 0.14005377, 0.1534091 , 1.        , 0.1559068 ,\n",
       "        0.1424716 , 0.1418383 , 0.14263596, 0.15862083, 0.13665866],\n",
       "       [0.13864201, 0.13739586, 0.14080902, 0.1559068 , 1.        ,\n",
       "        0.13782362, 0.13879806, 0.13964768, 0.20998865, 0.13728369],\n",
       "       [0.14442935, 0.15046942, 0.14244372, 0.1424716 , 0.13782362,\n",
       "        1.        , 0.14224166, 0.14178897, 0.14487185, 0.4820613 ],\n",
       "       [0.14075505, 0.14217691, 0.14985849, 0.1418383 , 0.13879806,\n",
       "        0.14224166, 1.        , 0.25023642, 0.15119474, 0.13771769],\n",
       "       [0.13886153, 0.13986649, 0.18817661, 0.14263596, 0.13964768,\n",
       "        0.14178897, 0.25023642, 1.        , 0.15836137, 0.1382584 ],\n",
       "       [0.14254869, 0.15949888, 0.15472162, 0.15862083, 0.20998865,\n",
       "        0.14487185, 0.15119474, 0.15836137, 1.        , 0.1405226 ],\n",
       "       [0.16894208, 0.16752942, 0.13948326, 0.13665866, 0.13728369,\n",
       "        0.4820613 , 0.13771769, 0.1382584 , 0.1405226 , 1.        ]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.affinity_matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "# train = lr.fit(X_train_tfidf, y_train_tfidf)\n",
    "# print('Training set score:', lr.score(X_train_tfidf, y_train_tfidf))\n",
    "# print('Test set score:', lr.score(X_test_tfidf, y_test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7803d25e5854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Run SVD on the training data, then project the training data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mX_train_lsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mvariance_explained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 7543 to 950.\n",
    "svd= TruncatedSVD(850)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "sents_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(3):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(sents_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_by_component.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svd, X_test_svd, y_train_svd, y_test_svd = train_test_split(sents_by_component, y_train, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with tf idf\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train_tfidf, y_train_tfidf)\n",
    "print('Training set score:', rfc.score(X_train_tfidf, y_train_tfidf))\n",
    "print('Test set score:', rfc.score(X_test_tfidf, y_test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with SVD\n",
    "train = rfc.fit(X_train_svd, y_train_svd)\n",
    "print('Training set score:', rfc.score(X_train_svd, y_train_svd))\n",
    "print('Test set score:', rfc.score(X_test_svd, y_test_svd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
