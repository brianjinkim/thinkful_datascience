{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    return [re.sub(r'--', '', word) for word in text]\n",
    "\n",
    "def raw_text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].+?[\\]]\", \"\", text)\n",
    " \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_spacy(list_of_docs, max_length=2000000):\n",
    "    \n",
    "    # load spacy\n",
    "    print('Running Spacy...')\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.max_length = max_length\n",
    "    \n",
    "    # set empty list; holds processed list of docs\n",
    "    nlp_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(list_of_docs):\n",
    "        print('Processing file {}'.format(i+1))\n",
    "        nlp_docs.append(nlp(doc))\n",
    "    print('Done processing')\n",
    "    return nlp_docs\n",
    "\n",
    "# put the tokens of a doc in a list\n",
    "def convert_to_tokens(doc):\n",
    "  token_list = []\n",
    "  for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "      token_list.append(token.lemma_.lower())\n",
    "  return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting documents..\n",
      "Processing austen-emma.txt\n",
      "Processing austen-persuasion.txt\n",
      "Processing austen-sense.txt\n",
      "Processing bible-kjv.txt\n",
      "Processing blake-poems.txt\n",
      "Processing bryant-stories.txt\n",
      "Processing burgess-busterbrown.txt\n",
      "Processing carroll-alice.txt\n",
      "Processing chesterton-ball.txt\n",
      "Processing chesterton-brown.txt\n",
      "Processing chesterton-thursday.txt\n",
      "Processing edgeworth-parents.txt\n",
      "Processing melville-moby_dick.txt\n",
      "Processing milton-paradise.txt\n",
      "Processing shakespeare-caesar.txt\n",
      "Processing shakespeare-hamlet.txt\n",
      "Processing shakespeare-macbeth.txt\n",
      "Processing whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "doc_names, doc_titles, docs = [], [], []\n",
    "for name in gutenberg.fileids():\n",
    "    doc_titles.append(str(name))\n",
    "\n",
    "  \n",
    "print('Getting documents..')\n",
    "for name in doc_titles:\n",
    "    clean_doc = gutenberg.paras(name)\n",
    "    print('Processing',name)\n",
    "    paras = []\n",
    "    for paragraph in clean_doc:\n",
    "        para = paragraph[0]\n",
    "        para = text_cleaner(para)\n",
    "        docs.append(' '.join(para))\n",
    "        doc_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting documents..\n",
      "length of bible-kjv.txt (4305662) is too long, trimming.\n",
      "Running Spacy...\n",
      "Processing file 1\n",
      "Processing file 2\n",
      "Processing file 3\n",
      "Processing file 4\n",
      "Processing file 5\n",
      "Processing file 6\n",
      "Processing file 7\n",
      "Processing file 8\n",
      "Processing file 9\n",
      "Processing file 10\n",
      "Processing file 11\n",
      "Processing file 12\n",
      "Processing file 13\n",
      "Processing file 14\n",
      "Processing file 15\n",
      "Processing file 16\n",
      "Processing file 17\n",
      "Processing file 18\n",
      "Done processing\n",
      "Adding tokens..\n",
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "print('Getting documents..')\n",
    "raw_docs, all_tokens = [], []\n",
    "for name in doc_titles:\n",
    "    clean_doc = raw_text_cleaner(gutenberg.raw(name))\n",
    "    if len(clean_doc) >= 2000000:\n",
    "        print('length of {} ({}) is too long, trimming.'.format(name, len(clean_doc)))\n",
    "        clean_doc = clean_doc[:1500000]\n",
    "    raw_docs.append(clean_doc)\n",
    "\n",
    "nlp_processed = load_spacy(raw_docs)\n",
    "print('Adding tokens..')\n",
    "for doc in nlp_processed:\n",
    "    all_tokens.append(convert_to_tokens(doc))\n",
    "print('Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(docs, doc_names, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "# Apply the vectorizer\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 13332\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features: %d\" % X_train.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9663441459000418\n",
      "\n",
      "Test set score: 0.7777081701905507\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8691702631212586\n",
      "\n",
      "Test set score: 0.8092926128948056\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.826743700403731\n",
      "\n",
      "Test set score: 0.7563560428086662\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 3), (3, 72), (4, 1), (5, 7), (6, 3), (7, 8), (8, 3), (9, 5), (10, 1), (11, 16), (12, 12), (13, 4), (14, 33)]\n",
      "[(1, 1), (2, 3), (3, 30), (4, 1), (5, 1), (6, 1), (7, 5), (11, 9), (12, 5), (13, 3), (14, 5), (16, 6), (17, 1), (18, 1), (21, 3)]\n",
      "[(0, 3), (2, 12), (3, 47), (4, 5), (6, 9), (7, 9), (9, 3), (10, 1), (11, 11), (12, 3), (13, 3), (14, 10), (16, 2), (17, 4), (18, 2)]\n",
      "[(0, 15), (1, 31), (2, 1), (3, 55), (4, 39), (5, 6), (6, 3), (7, 33), (12, 1), (18, 9), (19, 1), (20, 7), (21, 2), (23, 21), (24, 2)]\n",
      "[(7, 1), (40, 1), (63, 1), (96, 1), (98, 7), (102, 6), (103, 1), (107, 1), (110, 1), (118, 1), (137, 1), (138, 3), (139, 11), (146, 1), (147, 4)]\n",
      "[(1, 1), (3, 13), (6, 2), (12, 1), (15, 1), (16, 1), (19, 1), (37, 2), (40, 1), (42, 1), (48, 5), (49, 3), (55, 1), (58, 2), (66, 2)]\n",
      "[(2, 1), (3, 1), (23, 1), (34, 1), (43, 2), (53, 4), (65, 3), (71, 2), (73, 6), (74, 1), (88, 40), (89, 2), (94, 3), (96, 4), (98, 3)]\n",
      "[(1, 1), (3, 1), (11, 1), (16, 2), (25, 1), (26, 2), (28, 1), (37, 2), (40, 1), (48, 1), (53, 1), (55, 24), (58, 3), (68, 3), (70, 3)]\n",
      "[(1, 2), (2, 1), (3, 8), (6, 2), (7, 1), (8, 8), (9, 23), (10, 2), (11, 1), (12, 2), (13, 3), (14, 4), (15, 5), (16, 5), (22, 6)]\n",
      "[(1, 2), (2, 2), (3, 4), (5, 1), (6, 1), (8, 5), (9, 14), (10, 1), (11, 6), (12, 8), (13, 1), (14, 3), (15, 1), (16, 7), (22, 4)]\n",
      "[(1, 1), (3, 5), (6, 4), (8, 6), (9, 10), (10, 1), (11, 2), (12, 4), (13, 2), (14, 1), (16, 5), (17, 1), (21, 1), (22, 9), (23, 7)]\n",
      "[(2, 9), (3, 39), (6, 12), (7, 7), (9, 4), (11, 8), (12, 1), (13, 3), (14, 12), (17, 1), (18, 2), (21, 1), (22, 1), (23, 13), (25, 1)]\n",
      "[(0, 3), (1, 6), (2, 1), (3, 9), (4, 2), (5, 3), (6, 6), (7, 6), (9, 2), (11, 5), (12, 10), (13, 4), (14, 3), (15, 6), (16, 3)]\n",
      "[(0, 5), (1, 5), (3, 6), (4, 3), (5, 1), (6, 14), (7, 1), (8, 1), (11, 5), (12, 5), (13, 8), (14, 1), (18, 3), (19, 2), (20, 1)]\n",
      "[(1, 2), (6, 3), (7, 2), (11, 1), (12, 1), (21, 2), (34, 3), (43, 1), (48, 1), (49, 2), (55, 2), (56, 1), (80, 1), (89, 1), (96, 1)]\n",
      "[(0, 1), (6, 3), (7, 1), (12, 1), (13, 1), (16, 2), (21, 2), (22, 2), (23, 1), (26, 3), (34, 3), (37, 2), (38, 1), (43, 1), (45, 1)]\n",
      "[(1, 2), (7, 2), (11, 2), (12, 1), (13, 3), (21, 2), (31, 1), (34, 2), (37, 1), (43, 1), (48, 6), (49, 2), (55, 1), (56, 2), (80, 1)]\n",
      "[(1, 1), (2, 1), (3, 5), (6, 1), (7, 1), (8, 3), (9, 2), (11, 2), (12, 3), (13, 7), (14, 3), (15, 24), (22, 1), (23, 27), (24, 1)]\n"
     ]
    }
   ],
   "source": [
    "# create dictionary from list of document\n",
    "dic = gensim.corpora.Dictionary(all_tokens)\n",
    "dic.filter_n_most_frequent(3)\n",
    "# don't want words that exist in almost all documents\n",
    "dic.filter_extremes(no_above=0.95)\n",
    "\n",
    "# create bag of words representation for each document\n",
    "corpus = [dic.doc2bow(token) for token in all_tokens]\n",
    "for doc in corpus:\n",
    "    print(doc[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5421"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [value[1] for value in dic.items()]\n",
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "# name to add name to source column in row\n",
    "for i, doc in enumerate(nlp_docs):\n",
    "    print('Processing document {}'.format(doc_titles[i]))\n",
    "    # document level, searching by sentence\n",
    "    for sentence in doc.sents:\n",
    "        sentence_list = []\n",
    "        # word is in column\n",
    "        for word in columns:\n",
    "            if word in str(sentence):\n",
    "                sentence_list.append(1)\n",
    "            else:\n",
    "                sentence_list.append(0)\n",
    "        \n",
    "        # now append source since columns are done\n",
    "        sentence_list.append(doc_titles[i])\n",
    "        \n",
    "        # append sentence_list as a row\n",
    "        rows_list.append(list(sentence_list))\n",
    "\n",
    "\n",
    "print(\"--- Fitted in %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df = pd.DataFrame(rows_list, columns=(columns + ['source']))\n",
    "print(df.head())\n",
    "print(\"--- Generated dataframe in %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
