{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    return [re.sub(r'--', '', word) for word in text]\n",
    "\n",
    "def raw_text_cleaner(text):\n",
    "    text = re.sub(r'──────', ' ', text)\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"\\[\", \"\", text)\n",
    "    text = re.sub(\"\\]\", \"\", text)\n",
    "    text = re.sub(\"\\\\\\\\\\\\\\\\\", \"\", text)\n",
    "    text = re.sub(\"\\\\\\\\\", \"\", text)\n",
    " \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_spacy(list_of_docs, max_length=2000000):\n",
    "    \n",
    "    # load spacy\n",
    "    print('Running Spacy...')\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.max_length = max_length\n",
    "    \n",
    "    # set empty list; holds processed list of docs\n",
    "    nlp_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(list_of_docs):\n",
    "        print('Processing file {}'.format(i+1))\n",
    "        doc = raw_text_cleaner(doc)\n",
    "        nlp_docs.append(nlp(doc))\n",
    "    print('Done processing')\n",
    "    return nlp_docs\n",
    "\n",
    "# put the tokens of a doc in a list\n",
    "def convert_to_tokens(doc):\n",
    "  token_list = []\n",
    "  for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "      token_list.append(token.lemma_.lower())\n",
    "  return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading files\n",
    "filenames = ['against_the_gods', 'battle_through_the_heavens', 'desolate_era', 'emperors_domination', 'martial_god_asura', 'martial_world', 'overgeared', 'praise_the_orc', 'sovereign_of_the_three_realms', 'wu_dong_qian_kun']\n",
    "raw_files = []\n",
    "\n",
    "for filename in filenames:\n",
    "    with open('../dataset/' + filename + '.txt', encoding='utf-8') as myfile:\n",
    "        raw_files.append(myfile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183613\n",
      "135767\n",
      "198176\n",
      "137377\n",
      "132773\n",
      "201182\n",
      "167151\n",
      "170911\n",
      "196830\n",
      "155296\n"
     ]
    }
   ],
   "source": [
    "# check length of files\n",
    "for file in raw_files:\n",
    "    print(len(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spacy...\n",
      "Processing file 1\n",
      "Processing file 2\n",
      "Processing file 3\n",
      "Processing file 4\n",
      "Processing file 5\n",
      "Processing file 6\n",
      "Processing file 7\n",
      "Processing file 8\n",
      "Processing file 9\n",
      "Processing file 10\n",
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "# run spacy\n",
    "processed_files = load_spacy(raw_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences and compile\n",
    "all_sentences = []\n",
    "all_sentences_label = []\n",
    "for i, file in enumerate(processed_files):\n",
    "    for sentence in file.sents:\n",
    "        all_sentences.append(str(sentence))\n",
    "        all_sentences_label.append(filenames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24287 sentences\n"
     ]
    }
   ],
   "source": [
    "print('There are', len(all_sentences), 'sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.75, # drop words that occur in more than 3/4 the sentence\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #don't convert everything to lower case\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "# Apply the vectorizer\n",
    "vec_all_sents = vectorizer.fit_transform(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 7543\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features: %d\" % vec_all_sents.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and test sets\n",
    "# keeps sentence structure\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_sentences, all_sentences_label, test_size=0.4, random_state=0)\n",
    "# scores of tfidf\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(vec_all_sents, all_sentences_label, test_size=0.4, random_state=0)\n",
    "\n",
    "# force output into compressed sparse row if it isn't already; readable format\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG sentence: If you selected an orc, can you really endure it?\n",
      "tf-idf vec: {'orc': 0.38768266100540216, 'selected': 0.615361908167517, 'endure': 0.5663338509112565, 'really': 0.38768266100540216}\n"
     ]
    }
   ],
   "source": [
    "n = X_train_tfidf_csr.shape[0]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# create empty list of dictionary, per sentence\n",
    "sents_tfidf = [{} for _ in range(0,n)]\n",
    "\n",
    "# for each sentence, list feature words and tf-idf score\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    sents_tfidf[i][terms[j]] = X_train_tfidf_csr[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG sentence: If you selected an orc, can you really endure it?\n",
      "tf-idf vec: {'orc': 0.38768266100540216, 'selected': 0.615361908167517, 'endure': 0.5663338509112565, 'really': 0.38768266100540216}\n"
     ]
    }
   ],
   "source": [
    "# log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in this corpus.\n",
    "print('OG sentence:', X_train[3])\n",
    "print('tf-idf vec:', sents_tfidf[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The tiger's body flying through the air was immensely large, causing even Ian to flinch for a moment. \""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[8][:102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tiger\\'s body flying through the air was immensely large, causing even Ian to flinch for a moment. \"'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[8][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8110214686175304\n",
      "Test set score: 0.6886473182237719\n"
     ]
    }
   ],
   "source": [
    "# lr = LogisticRegression()\n",
    "# train = lr.fit(X_train_tfidf, y_train_tfidf)\n",
    "# print('Training set score:', lr.score(X_train_tfidf, y_train_tfidf))\n",
    "# print('Test set score:', lr.score(X_test_tfidf, y_test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 55.60112867653755\n",
      "Component 0:\n",
      "Lin Xiao remarked. \"                                                                                                0.561253\n",
      "Lin Xiao said.                                                                                                      0.502035\n",
      "This was Lin Dong's father, Lin Xiao.                                                                               0.483478\n",
      "This was because Lin Xiao was injured once again...                                                                 0.468415\n",
      "This was because Lin Xiao was injured once again...                                                                 0.468415\n",
      "Lin Xiao extended his palm and signalled to Lin Dong:                                                               0.437992\n",
      "Looking at Lin Dong with doting eyes, she understood that Lin Xiao had invested everything he had into Lin Dong.    0.429803\n",
      "Lin Xiao looked Lin Dong in the eye and asked him excitedly.                                                        0.419330\n",
      "Lin Xiao smiled as he said.                                                                                         0.418965\n",
      "Lin Dong frowned, the third uncle mentioned by Lin Changqiang was his father Lin Xiao.                              0.415932\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "Chu Feng.                                                                                                                                        0.776801\n",
      "Chu Feng.                                                                                                                                        0.776801\n",
      "Chu Yue explained in detail to Chu Feng.                                                                                                         0.761774\n",
      "Chu Feng was astonished.                                                                                                                         0.754023\n",
      "Chu Guyu.                                                                                                                                        0.751575\n",
      "If Chu Feng agreed, Chu Zhen could openly give Chu Feng a lesson.                                                                                0.741549\n",
      "Chu Feng was incomparably joyful.                                                                                                                0.739042\n",
      "Chu Feng remembered that name because he was qualified to be remembered by Chu Feng.                                                             0.720895\n",
      "Chu Cheng shot Chu Yue a glance, then looked at Chu Feng, \"                                                                                      0.708323\n",
      "In the Chu family, excluding Chu Yuan, Chu Guyu and Chu Yue, pretty much no one liked Chu Feng and they all wanted him out of the Chu family.    0.708293\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "\"What?\"             0.980765\n",
      "\"What is this?\"     0.980765\n",
      "What?               0.980765\n",
      "What?\"              0.980765\n",
      "What?               0.980765\n",
      "What?!\"             0.980765\n",
      "\"What?\"             0.980765\n",
      "What about you?\"    0.980765\n",
      "\"What's that?\"      0.980765\n",
      "\"What's this?       0.980765\n",
      "Name: 2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 7543 to 950.\n",
    "svd= TruncatedSVD(850)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "sents_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(3):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(sents_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>840</th>\n",
       "      <th>841</th>\n",
       "      <th>842</th>\n",
       "      <th>843</th>\n",
       "      <th>844</th>\n",
       "      <th>845</th>\n",
       "      <th>846</th>\n",
       "      <th>847</th>\n",
       "      <th>848</th>\n",
       "      <th>849</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jiang Chen held many profound and flowery conversations with the visitors before finally agreeing to treat the princess in three days.</th>\n",
       "      <td>0.055208</td>\n",
       "      <td>0.007985</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>0.024521</td>\n",
       "      <td>0.042389</td>\n",
       "      <td>0.025492</td>\n",
       "      <td>0.025370</td>\n",
       "      <td>-0.037478</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016571</td>\n",
       "      <td>0.029853</td>\n",
       "      <td>-0.007111</td>\n",
       "      <td>0.009331</td>\n",
       "      <td>-0.014426</td>\n",
       "      <td>-0.002391</td>\n",
       "      <td>0.016366</td>\n",
       "      <td>-0.016610</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.002426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Another disciple of the sect scornfully laughed: \"Heh, the Cleansing Incense Ancient Sect is only a second-rate establishment.</th>\n",
       "      <td>0.041384</td>\n",
       "      <td>-0.007311</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.031205</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016054</td>\n",
       "      <td>-0.001880</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>-0.010760</td>\n",
       "      <td>0.016136</td>\n",
       "      <td>-0.012841</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.006339</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>-0.006582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It was a defense type magic that could be acquired with a black magician reached level 230.</th>\n",
       "      <td>0.077130</td>\n",
       "      <td>0.006550</td>\n",
       "      <td>0.014536</td>\n",
       "      <td>0.153935</td>\n",
       "      <td>0.206566</td>\n",
       "      <td>-0.087973</td>\n",
       "      <td>-0.016190</td>\n",
       "      <td>-0.023685</td>\n",
       "      <td>-0.004810</td>\n",
       "      <td>-0.003911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017051</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>-0.020635</td>\n",
       "      <td>0.008843</td>\n",
       "      <td>-0.004224</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>-0.037168</td>\n",
       "      <td>0.033563</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>-0.030618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If you selected an orc, can you really endure it?</th>\n",
       "      <td>0.073680</td>\n",
       "      <td>0.008999</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.048429</td>\n",
       "      <td>0.046273</td>\n",
       "      <td>0.068411</td>\n",
       "      <td>0.027307</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>-0.025217</td>\n",
       "      <td>0.025685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034595</td>\n",
       "      <td>-0.042489</td>\n",
       "      <td>-0.005446</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>0.019061</td>\n",
       "      <td>-0.025807</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>0.025350</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>-0.017841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>At the time, I thought that it was just a baseless rumour, but now I realize that it's actually true.</th>\n",
       "      <td>0.103623</td>\n",
       "      <td>0.016841</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>0.030889</td>\n",
       "      <td>0.037032</td>\n",
       "      <td>0.024744</td>\n",
       "      <td>0.043649</td>\n",
       "      <td>0.022207</td>\n",
       "      <td>-0.013411</td>\n",
       "      <td>0.043325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.020512</td>\n",
       "      <td>-0.015124</td>\n",
       "      <td>0.009032</td>\n",
       "      <td>0.009049</td>\n",
       "      <td>-0.001825</td>\n",
       "      <td>-0.005820</td>\n",
       "      <td>-0.008339</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.008272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 850 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0         1    \\\n",
       "Jiang Chen held many profound and flowery conve...  0.055208  0.007985   \n",
       "Another disciple of the sect scornfully laughed...  0.041384 -0.007311   \n",
       "It was a defense type magic that could be acqui...  0.077130  0.006550   \n",
       "If you selected an orc, can you really endure it?   0.073680  0.008999   \n",
       "At the time, I thought that it was just a basel...  0.103623  0.016841   \n",
       "\n",
       "                                                         2         3    \\\n",
       "Jiang Chen held many profound and flowery conve...  0.002144  0.018281   \n",
       "Another disciple of the sect scornfully laughed...  0.002530  0.029153   \n",
       "It was a defense type magic that could be acqui...  0.014536  0.153935   \n",
       "If you selected an orc, can you really endure it?   0.006270  0.048429   \n",
       "At the time, I thought that it was just a basel...  0.005413  0.030889   \n",
       "\n",
       "                                                         4         5    \\\n",
       "Jiang Chen held many profound and flowery conve...  0.024521  0.042389   \n",
       "Another disciple of the sect scornfully laughed...  0.002118  0.031205   \n",
       "It was a defense type magic that could be acqui...  0.206566 -0.087973   \n",
       "If you selected an orc, can you really endure it?   0.046273  0.068411   \n",
       "At the time, I thought that it was just a basel...  0.037032  0.024744   \n",
       "\n",
       "                                                         6         7    \\\n",
       "Jiang Chen held many profound and flowery conve...  0.025492  0.025370   \n",
       "Another disciple of the sect scornfully laughed...  0.000226  0.000844   \n",
       "It was a defense type magic that could be acqui... -0.016190 -0.023685   \n",
       "If you selected an orc, can you really endure it?   0.027307  0.010658   \n",
       "At the time, I thought that it was just a basel...  0.043649  0.022207   \n",
       "\n",
       "                                                         8         9    \\\n",
       "Jiang Chen held many profound and flowery conve... -0.037478  0.013844   \n",
       "Another disciple of the sect scornfully laughed...  0.000365  0.007750   \n",
       "It was a defense type magic that could be acqui... -0.004810 -0.003911   \n",
       "If you selected an orc, can you really endure it?  -0.025217  0.025685   \n",
       "At the time, I thought that it was just a basel... -0.013411  0.043325   \n",
       "\n",
       "                                                      ...          840  \\\n",
       "Jiang Chen held many profound and flowery conve...    ...    -0.016571   \n",
       "Another disciple of the sect scornfully laughed...    ...     0.016054   \n",
       "It was a defense type magic that could be acqui...    ...     0.017051   \n",
       "If you selected an orc, can you really endure it?     ...    -0.034595   \n",
       "At the time, I thought that it was just a basel...    ...     0.006308   \n",
       "\n",
       "                                                         841       842  \\\n",
       "Jiang Chen held many profound and flowery conve...  0.029853 -0.007111   \n",
       "Another disciple of the sect scornfully laughed... -0.001880  0.006004   \n",
       "It was a defense type magic that could be acqui...  0.016200 -0.020635   \n",
       "If you selected an orc, can you really endure it?  -0.042489 -0.005446   \n",
       "At the time, I thought that it was just a basel...  0.020512 -0.015124   \n",
       "\n",
       "                                                         843       844  \\\n",
       "Jiang Chen held many profound and flowery conve...  0.009331 -0.014426   \n",
       "Another disciple of the sect scornfully laughed... -0.010760  0.016136   \n",
       "It was a defense type magic that could be acqui...  0.008843 -0.004224   \n",
       "If you selected an orc, can you really endure it?   0.008788  0.019061   \n",
       "At the time, I thought that it was just a basel...  0.009032  0.009049   \n",
       "\n",
       "                                                         845       846  \\\n",
       "Jiang Chen held many profound and flowery conve... -0.002391  0.016366   \n",
       "Another disciple of the sect scornfully laughed... -0.012841  0.000258   \n",
       "It was a defense type magic that could be acqui...  0.011871 -0.037168   \n",
       "If you selected an orc, can you really endure it?  -0.025807  0.029852   \n",
       "At the time, I thought that it was just a basel... -0.001825 -0.005820   \n",
       "\n",
       "                                                         847       848  \\\n",
       "Jiang Chen held many profound and flowery conve... -0.016610  0.001388   \n",
       "Another disciple of the sect scornfully laughed...  0.006339  0.008038   \n",
       "It was a defense type magic that could be acqui...  0.033563  0.021943   \n",
       "If you selected an orc, can you really endure it?   0.025350  0.002656   \n",
       "At the time, I thought that it was just a basel... -0.008339  0.015010   \n",
       "\n",
       "                                                         849  \n",
       "Jiang Chen held many profound and flowery conve...  0.002426  \n",
       "Another disciple of the sect scornfully laughed... -0.006582  \n",
       "It was a defense type magic that could be acqui... -0.030618  \n",
       "If you selected an orc, can you really endure it?  -0.017841  \n",
       "At the time, I thought that it was just a basel...  0.008272  \n",
       "\n",
       "[5 rows x 850 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_by_component.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svd, X_test_svd, y_train_svd, y_test_svd = train_test_split(sents_by_component, y_train, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9626681306615427\n",
      "Test set score: 0.6429233144621719\n"
     ]
    }
   ],
   "source": [
    "# random forest with tf idf\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train_tfidf, y_train_tfidf)\n",
    "print('Training set score:', rfc.score(X_train_tfidf, y_train_tfidf))\n",
    "print('Test set score:', rfc.score(X_test_tfidf, y_test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9687750200160128\n",
      "Test set score: 0.4570252187339166\n"
     ]
    }
   ],
   "source": [
    "# random forest with SVD\n",
    "train = rfc.fit(X_train_svd, y_train_svd)\n",
    "print('Training set score:', rfc.score(X_train_svd, y_train_svd))\n",
    "print('Test set score:', rfc.score(X_test_svd, y_test_svd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
