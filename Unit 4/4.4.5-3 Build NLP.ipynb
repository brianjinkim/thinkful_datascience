{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import ensemble\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    # Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    \n",
    "    # Get rid of headings in square brackets.\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    # Get rid of chapter titles.\n",
    "    text = re.sub(r'Chapter \\d+','',text)\n",
    "    text = re.sub(r'CHAPTER \\d+', '', text)\n",
    "    text = re.sub(r'Chapter \\w+','',text)\n",
    "    text = re.sub(r'CHAPTER \\w+', '', text)\n",
    "    text = re.sub(\"\\\\n\\\\n.*?\\\\n\\\\n\", '', text)\n",
    "  \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "# returns list of documents with NLP\n",
    "def load_spacy(list_of_docs):\n",
    "    \n",
    "    # load spacy\n",
    "    print('Running Spacy...')\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.max_length = 2000000\n",
    "    \n",
    "    # set empty list; holds processed list of docs\n",
    "    nlp_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(list_of_docs):\n",
    "      print('Processing {}'.format(doc_names[i]))\n",
    "      nlp_docs.append(nlp(doc))\n",
    "    print('Done processing')\n",
    "    return nlp_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting documents..\n",
      "length of bible-kjv.txt (4227119) is too long, trimming.\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "doc_names, docs = [], []\n",
    "for name in gutenberg.fileids():\n",
    "  doc_names.append(str(name))\n",
    "  \n",
    "print('Getting documents..')\n",
    "for name in doc_names:\n",
    "  clean_doc = text_cleaner(gutenberg.raw(name))\n",
    "  if len(clean_doc) >= 2000000:\n",
    "    print('length of {} ({}) is too long, trimming.'.format(name, len(clean_doc)))\n",
    "    clean_doc = clean_doc[:1500000]\n",
    "  docs.append(clean_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spacy...\n",
      "Processing austen-emma.txt\n",
      "Processing austen-persuasion.txt\n",
      "Processing austen-sense.txt\n",
      "Processing bible-kjv.txt\n",
      "Processing blake-poems.txt\n",
      "Processing bryant-stories.txt\n",
      "Processing burgess-busterbrown.txt\n",
      "Processing carroll-alice.txt\n",
      "Processing chesterton-ball.txt\n",
      "Processing chesterton-brown.txt\n",
      "Processing chesterton-thursday.txt\n",
      "Processing edgeworth-parents.txt\n",
      "Processing melville-moby_dick.txt\n",
      "Processing milton-paradise.txt\n",
      "Processing shakespeare-caesar.txt\n",
      "Processing shakespeare-hamlet.txt\n",
      "Processing shakespeare-macbeth.txt\n",
      "Processing whitman-leaves.txt\n",
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "# # run spacy\n",
    "# print('Running Spacy...')\n",
    "# nlp = spacy.load('en')\n",
    "# nlp.max_length = 2000000\n",
    "# for i, doc in enumerate(docs):\n",
    "#   print('Processing {}'.format(doc_names[i]))\n",
    "#   nlp_docs.append(nlp(doc))\n",
    "# print('Done processing')\n",
    "\n",
    "nlp_docs = load_spacy(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the tokens of a doc in a list\n",
    "def convert_to_tokens(doc):\n",
    "  token_list = []\n",
    "  for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "      token_list.append(token.lemma_.lower())\n",
    "  return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of documents tokens and sentences\n",
    "all_docs = []\n",
    "\n",
    "for doc in nlp_docs:\n",
    "    all_docs.append(convert_to_tokens(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 3), (3, 72), (4, 1), (5, 7), (6, 3), (7, 8), (8, 3), (9, 5), (10, 1), (11, 16), (12, 12), (13, 4), (14, 33)]\n",
      "[(1, 1), (2, 3), (3, 30), (4, 1), (5, 1), (6, 1), (7, 5), (11, 9), (12, 5), (13, 3), (14, 5), (16, 6), (17, 1), (18, 1), (21, 3)]\n",
      "[(0, 3), (2, 12), (3, 47), (4, 5), (6, 8), (7, 9), (9, 3), (10, 1), (11, 11), (12, 3), (13, 3), (14, 10), (16, 2), (17, 4), (18, 2)]\n",
      "[(0, 15), (1, 31), (2, 1), (3, 55), (4, 38), (5, 5), (6, 3), (7, 33), (12, 1), (18, 9), (19, 1), (20, 7), (21, 2), (23, 21), (24, 2)]\n",
      "[(7, 1), (40, 1), (63, 1), (96, 1), (98, 7), (102, 6), (107, 1), (110, 1), (117, 1), (136, 1), (137, 3), (138, 11), (145, 1), (146, 4), (151, 1)]\n",
      "[(1, 1), (3, 13), (6, 2), (12, 1), (15, 1), (16, 1), (19, 1), (37, 2), (40, 1), (42, 1), (48, 5), (49, 3), (55, 1), (58, 2), (66, 2)]\n",
      "[(2, 1), (3, 1), (23, 1), (34, 1), (43, 2), (53, 4), (65, 3), (71, 2), (73, 6), (74, 1), (88, 40), (89, 2), (94, 3), (96, 4), (98, 3)]\n",
      "[(1, 1), (3, 1), (11, 1), (16, 2), (25, 1), (26, 2), (28, 1), (37, 2), (40, 1), (48, 1), (53, 1), (55, 24), (58, 3), (68, 3), (70, 3)]\n",
      "[(1, 2), (2, 1), (3, 8), (6, 1), (7, 1), (8, 8), (9, 23), (10, 2), (11, 1), (12, 2), (13, 3), (14, 4), (15, 5), (16, 5), (22, 6)]\n",
      "[(1, 2), (2, 2), (3, 4), (5, 1), (6, 1), (8, 5), (9, 14), (10, 1), (11, 5), (12, 7), (13, 1), (14, 3), (15, 1), (16, 7), (22, 4)]\n",
      "[(1, 1), (3, 5), (6, 3), (8, 6), (9, 8), (10, 1), (11, 2), (12, 4), (13, 1), (14, 1), (16, 5), (17, 1), (21, 1), (22, 7), (23, 7)]\n",
      "[(2, 9), (3, 39), (6, 12), (7, 7), (9, 4), (11, 8), (12, 1), (13, 3), (14, 12), (17, 1), (18, 2), (21, 1), (22, 1), (23, 13), (25, 1)]\n",
      "[(0, 3), (1, 6), (2, 1), (3, 9), (4, 2), (5, 3), (6, 6), (7, 6), (9, 2), (11, 5), (12, 10), (13, 4), (14, 3), (15, 6), (16, 3)]\n",
      "[(0, 5), (1, 5), (3, 6), (4, 3), (5, 1), (6, 14), (7, 1), (8, 1), (11, 5), (12, 5), (13, 8), (14, 1), (18, 3), (19, 2), (20, 1)]\n",
      "[(1, 1), (6, 3), (7, 2), (11, 1), (12, 1), (21, 2), (34, 3), (43, 1), (48, 1), (49, 2), (55, 2), (56, 1), (80, 1), (89, 1), (96, 1)]\n",
      "[(0, 1), (6, 3), (7, 1), (12, 1), (13, 1), (16, 2), (21, 2), (22, 2), (23, 1), (26, 3), (34, 2), (37, 2), (38, 1), (43, 1), (45, 1)]\n",
      "[(1, 2), (7, 2), (11, 2), (12, 1), (13, 3), (21, 2), (31, 1), (34, 2), (37, 1), (43, 1), (48, 6), (49, 2), (55, 1), (56, 2), (80, 1)]\n",
      "[(1, 1), (2, 1), (3, 5), (6, 1), (7, 1), (8, 3), (9, 2), (11, 2), (12, 3), (13, 7), (14, 3), (15, 24), (22, 1), (23, 27), (24, 1)]\n"
     ]
    }
   ],
   "source": [
    "# create dictionary from list of document\n",
    "dic = gensim.corpora.Dictionary(all_docs)\n",
    "dic.filter_n_most_frequent(3)\n",
    "# don't want words that exist in almost all documents\n",
    "dic.filter_extremes(no_above=0.95)\n",
    "\n",
    "# create bag of words representation for each document\n",
    "corpus = [dic.doc2bow(doc) for doc in all_docs]\n",
    "for doc in corpus:\n",
    "  print(doc[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [value[1] for value in dic.items()]\n",
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document austen-emma.txt\n",
      "Processing document austen-persuasion.txt\n",
      "Processing document austen-sense.txt\n",
      "Processing document bible-kjv.txt\n",
      "Processing document blake-poems.txt\n",
      "Processing document bryant-stories.txt\n",
      "Processing document burgess-busterbrown.txt\n",
      "Processing document carroll-alice.txt\n",
      "Processing document chesterton-ball.txt\n",
      "Processing document chesterton-brown.txt\n",
      "Processing document chesterton-thursday.txt\n",
      "Processing document edgeworth-parents.txt\n",
      "Processing document melville-moby_dick.txt\n",
      "Processing document milton-paradise.txt\n",
      "Processing document shakespeare-caesar.txt\n",
      "Processing document shakespeare-hamlet.txt\n",
      "Processing document shakespeare-macbeth.txt\n",
      "Processing document whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "rows_list = []\n",
    "\n",
    "# name to add name to source column in row\n",
    "for i, doc in enumerate(nlp_docs):\n",
    "    print('Processing document {}'.format(doc_names[i]))\n",
    "    # document level, searching by sentence\n",
    "    for sentence in doc.sents:\n",
    "        sentence_list = []\n",
    "        for word in columns:\n",
    "            if word in str(sentence):\n",
    "                sentence_list.append(1)\n",
    "            else:\n",
    "                sentence_list.append(0)\n",
    "        \n",
    "        # now append source since columns are done\n",
    "        sentence_list.append(doc_names[i])\n",
    "        \n",
    "        # append sentence_list as a row\n",
    "        rows_list.append(list(sentence_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 5385), indices imply (2, 5385)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   4856\u001b[0m                 blocks = [make_block(values=blocks[0],\n\u001b[1;32m-> 4857\u001b[1;33m                                      placement=slice(0, len(axes[0])))]\n\u001b[0m\u001b[0;32m   4858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[0;32m   3204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3205\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m   2302\u001b[0m         super(ObjectBlock, self).__init__(values, ndim=ndim,\n\u001b[1;32m-> 2303\u001b[1;33m                                           placement=placement)\n\u001b[0m\u001b[0;32m   2304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;34m'Wrong number of items passed {val}, placement implies '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m                 '{mgr}'.format(val=len(self.values), mgr=len(self.mgr_locs)))\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 1, placement implies 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5fc8810549aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'source'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    401\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                     mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[1;32m--> 403\u001b[1;33m                                              copy=copy)\n\u001b[0m\u001b[0;32m    404\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[1;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   4864\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'values'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4865\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4866\u001b[1;33m         \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[1;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[0;32m   4841\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4842\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[1;32m-> 4843\u001b[1;33m         passed, implied))\n\u001b[0m\u001b[0;32m   4844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (1, 5385), indices imply (2, 5385)"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(sentence_list, columns=[columns, 'source'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "\n",
    "# store tf-idf\n",
    "sims = gensim.similarities.Similarity('/usr/workdir/', tf_idf[corpus], num_features=len(dic))\n",
    "print(sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
