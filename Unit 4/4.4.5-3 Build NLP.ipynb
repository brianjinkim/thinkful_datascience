{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import ensemble\n",
    "import gensim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    # Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    \n",
    "    # Get rid of headings in square brackets.\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    # Get rid of chapter titles.\n",
    "    text = re.sub(r'Chapter \\d+','',text)\n",
    "    text = re.sub(r'CHAPTER \\d+', '', text)\n",
    "    text = re.sub(r'Chapter \\w+','',text)\n",
    "    text = re.sub(r'CHAPTER \\w+', '', text)\n",
    "    text = re.sub(\"\\\\n\\\\n.*?\\\\n\\\\n\", '', text)\n",
    "  \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "# returns list of documents with NLP\n",
    "def load_spacy(list_of_docs):\n",
    "    \n",
    "    # load spacy\n",
    "    print('Running Spacy...')\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.max_length = 2000000\n",
    "    \n",
    "    # set empty list; holds processed list of docs\n",
    "    nlp_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(list_of_docs):\n",
    "      print('Processing {}'.format(doc_names[i]))\n",
    "      nlp_docs.append(nlp(doc))\n",
    "    print('Done processing')\n",
    "    return nlp_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting documents..\n",
      "length of bible-kjv.txt (4227119) is too long, trimming.\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "doc_names, docs = [], []\n",
    "for name in gutenberg.fileids():\n",
    "  doc_names.append(str(name))\n",
    "# doc_names.append('austen-emma.txt')\n",
    "# doc_names.append('austen-persuasion.txt')\n",
    "# doc_names.append('austen-sense.txt')\n",
    "\n",
    "  \n",
    "print('Getting documents..')\n",
    "for name in doc_names:\n",
    "  clean_doc = text_cleaner(gutenberg.raw(name))\n",
    "  if len(clean_doc) >= 2000000:\n",
    "    print('length of {} ({}) is too long, trimming.'.format(name, len(clean_doc)))\n",
    "    clean_doc = clean_doc[:1500000]\n",
    "  docs.append(clean_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spacy...\n",
      "Processing austen-emma.txt\n",
      "Processing austen-persuasion.txt\n",
      "Processing austen-sense.txt\n",
      "Processing bible-kjv.txt\n",
      "Processing blake-poems.txt\n",
      "Processing bryant-stories.txt\n",
      "Processing burgess-busterbrown.txt\n",
      "Processing carroll-alice.txt\n",
      "Processing chesterton-ball.txt\n",
      "Processing chesterton-brown.txt\n",
      "Processing chesterton-thursday.txt\n",
      "Processing edgeworth-parents.txt\n",
      "Processing melville-moby_dick.txt\n",
      "Processing milton-paradise.txt\n",
      "Processing shakespeare-caesar.txt\n",
      "Processing shakespeare-hamlet.txt\n",
      "Processing shakespeare-macbeth.txt\n",
      "Processing whitman-leaves.txt\n",
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "# # run spacy\n",
    "# print('Running Spacy...')\n",
    "# nlp = spacy.load('en')\n",
    "# nlp.max_length = 2000000\n",
    "# for i, doc in enumerate(docs):\n",
    "#   print('Processing {}'.format(doc_names[i]))\n",
    "#   nlp_docs.append(nlp(doc))\n",
    "# print('Done processing')\n",
    "\n",
    "nlp_docs = load_spacy(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the tokens of a doc in a list\n",
    "def convert_to_tokens(doc):\n",
    "  token_list = []\n",
    "  for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "      token_list.append(token.lemma_.lower())\n",
    "  return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of documents tokens and sentences\n",
    "all_docs = []\n",
    "# all_sents = []\n",
    "# sentence_names = []\n",
    "\n",
    "for doc in nlp_docs:\n",
    "    all_docs.append(convert_to_tokens(doc))\n",
    "#     for sentence in doc.sents:\n",
    "#         all_sents.append(convert_to_tokens(sentence))\n",
    "#         sentence_names.append(str(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 3), (3, 72), (4, 1), (5, 7), (6, 3), (7, 8), (8, 3), (9, 5), (10, 1), (11, 16), (12, 12), (13, 4), (14, 33)]\n",
      "[(1, 1), (2, 3), (3, 30), (4, 1), (5, 1), (6, 1), (7, 5), (11, 9), (12, 5), (13, 3), (14, 5), (16, 6), (17, 1), (18, 1), (21, 3)]\n",
      "[(0, 3), (2, 12), (3, 47), (4, 5), (6, 8), (7, 9), (9, 3), (10, 1), (11, 11), (12, 3), (13, 3), (14, 10), (16, 2), (17, 4), (18, 2)]\n",
      "[(0, 15), (1, 31), (2, 1), (3, 55), (4, 38), (5, 5), (6, 3), (7, 33), (12, 1), (18, 9), (19, 1), (20, 7), (21, 2), (23, 21), (24, 2)]\n",
      "[(7, 1), (40, 1), (63, 1), (96, 1), (98, 7), (102, 6), (107, 1), (110, 1), (117, 1), (136, 1), (137, 3), (138, 11), (145, 1), (146, 4), (151, 1)]\n",
      "[(1, 1), (3, 13), (6, 2), (12, 1), (15, 1), (16, 1), (19, 1), (37, 2), (40, 1), (42, 1), (48, 5), (49, 3), (55, 1), (58, 2), (66, 2)]\n",
      "[(2, 1), (3, 1), (23, 1), (34, 1), (43, 2), (53, 4), (65, 3), (71, 2), (73, 6), (74, 1), (88, 40), (89, 2), (94, 3), (96, 4), (98, 3)]\n",
      "[(1, 1), (3, 1), (11, 1), (16, 2), (25, 1), (26, 2), (28, 1), (37, 2), (40, 1), (48, 1), (53, 1), (55, 24), (58, 3), (68, 3), (70, 3)]\n",
      "[(1, 2), (2, 1), (3, 8), (6, 1), (7, 1), (8, 8), (9, 23), (10, 2), (11, 1), (12, 2), (13, 3), (14, 4), (15, 5), (16, 5), (22, 6)]\n",
      "[(1, 2), (2, 2), (3, 4), (5, 1), (6, 1), (8, 5), (9, 14), (10, 1), (11, 5), (12, 7), (13, 1), (14, 3), (15, 1), (16, 7), (22, 4)]\n",
      "[(1, 1), (3, 5), (6, 3), (8, 6), (9, 8), (10, 1), (11, 2), (12, 4), (13, 1), (14, 1), (16, 5), (17, 1), (21, 1), (22, 7), (23, 7)]\n",
      "[(2, 9), (3, 39), (6, 12), (7, 7), (9, 4), (11, 8), (12, 1), (13, 3), (14, 12), (17, 1), (18, 2), (21, 1), (22, 1), (23, 13), (25, 1)]\n",
      "[(0, 3), (1, 6), (2, 1), (3, 9), (4, 2), (5, 3), (6, 6), (7, 6), (9, 2), (11, 5), (12, 10), (13, 4), (14, 3), (15, 6), (16, 3)]\n",
      "[(0, 5), (1, 5), (3, 6), (4, 3), (5, 1), (6, 14), (7, 1), (8, 1), (11, 5), (12, 5), (13, 8), (14, 1), (18, 3), (19, 2), (20, 1)]\n",
      "[(1, 1), (6, 3), (7, 2), (11, 1), (12, 1), (21, 2), (34, 3), (43, 1), (48, 1), (49, 2), (55, 2), (56, 1), (80, 1), (89, 1), (96, 1)]\n",
      "[(0, 1), (6, 3), (7, 1), (12, 1), (13, 1), (16, 2), (21, 2), (22, 2), (23, 1), (26, 3), (34, 2), (37, 2), (38, 1), (43, 1), (45, 1)]\n",
      "[(1, 2), (7, 2), (11, 2), (12, 1), (13, 3), (21, 2), (31, 1), (34, 2), (37, 1), (43, 1), (48, 6), (49, 2), (55, 1), (56, 2), (80, 1)]\n",
      "[(1, 1), (2, 1), (3, 5), (6, 1), (7, 1), (8, 3), (9, 2), (11, 2), (12, 3), (13, 7), (14, 3), (15, 24), (22, 1), (23, 27), (24, 1)]\n"
     ]
    }
   ],
   "source": [
    "# create dictionary from list of document\n",
    "dic = gensim.corpora.Dictionary(all_docs)\n",
    "dic.filter_n_most_frequent(3)\n",
    "# don't want words that exist in almost all documents\n",
    "dic.filter_extremes(no_above=0.95)\n",
    "\n",
    "# create bag of words representation for each document\n",
    "corpus = [dic.doc2bow(doc) for doc in all_docs]\n",
    "for doc in corpus:\n",
    "  print(doc[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [value[1] for value in dic.items()]\n",
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document austen-emma.txt\n",
      "Processing document austen-persuasion.txt\n",
      "Processing document austen-sense.txt\n",
      "Processing document bible-kjv.txt\n",
      "Processing document blake-poems.txt\n",
      "Processing document bryant-stories.txt\n",
      "Processing document burgess-busterbrown.txt\n",
      "Processing document carroll-alice.txt\n",
      "Processing document chesterton-ball.txt\n",
      "Processing document chesterton-brown.txt\n",
      "Processing document chesterton-thursday.txt\n",
      "Processing document edgeworth-parents.txt\n",
      "Processing document melville-moby_dick.txt\n",
      "Processing document milton-paradise.txt\n",
      "Processing document shakespeare-caesar.txt\n",
      "Processing document shakespeare-hamlet.txt\n",
      "Processing document shakespeare-macbeth.txt\n",
      "Processing document whitman-leaves.txt\n",
      "--- Fitted in 9555.635174512863 seconds ---\n"
     ]
    }
   ],
   "source": [
    "rows_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "# name to add name to source column in row\n",
    "for i, doc in enumerate(nlp_docs):\n",
    "    print('Processing document {}'.format(doc_names[i]))\n",
    "    # document level, searching by sentence\n",
    "    for sentence in doc.sents:\n",
    "        sentence_list = []\n",
    "        # word is in column\n",
    "        for word in columns:\n",
    "            if word in str(sentence):\n",
    "                sentence_list.append(1)\n",
    "            else:\n",
    "                sentence_list.append(0)\n",
    "        \n",
    "        # now append source since columns are done\n",
    "        sentence_list.append(doc_names[i])\n",
    "        \n",
    "        # append sentence_list as a row\n",
    "        rows_list.append(list(sentence_list))\n",
    "\n",
    "\n",
    "print(\"--- Fitted in %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abhor</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abode</th>\n",
       "      <th>abominable</th>\n",
       "      <th>about</th>\n",
       "      <th>abroad</th>\n",
       "      <th>abrupt</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>...</th>\n",
       "      <th>gods</th>\n",
       "      <th>norway</th>\n",
       "      <th>warlike</th>\n",
       "      <th>wherein</th>\n",
       "      <th>whereto</th>\n",
       "      <th>winds</th>\n",
       "      <th>ope</th>\n",
       "      <th>ore</th>\n",
       "      <th>seem'd</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>austen-emma.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>austen-emma.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>austen-emma.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>austen-emma.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>austen-emma.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abhor  abide  ability  able  abode  abominable  about  abroad  abrupt  \\\n",
       "0      0      0        0     1      0           0      0       0       0   \n",
       "1      0      0        0     0      0           0      0       0       0   \n",
       "2      0      0        0     0      0           0      0       0       0   \n",
       "3      0      0        0     0      0           0      0       0       0   \n",
       "4      0      0        0     0      0           0      0       0       0   \n",
       "\n",
       "   abruptly       ...         gods  norway  warlike  wherein  whereto  winds  \\\n",
       "0         0       ...            0       0        0        0        0      0   \n",
       "1         0       ...            0       0        0        0        0      0   \n",
       "2         0       ...            0       0        0        0        0      0   \n",
       "3         0       ...            0       0        0        0        0      0   \n",
       "4         0       ...            0       0        0        0        0      0   \n",
       "\n",
       "   ope  ore  seem'd           source  \n",
       "0    0    0       0  austen-emma.txt  \n",
       "1    0    0       0  austen-emma.txt  \n",
       "2    0    1       0  austen-emma.txt  \n",
       "3    0    0       0  austen-emma.txt  \n",
       "4    0    0       0  austen-emma.txt  \n",
       "\n",
       "[5 rows x 5385 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rows_list, columns=(columns + ['source']))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ItemsView' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d9cd02e3caa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtf_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument_freq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- Fitted in %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-d9cd02e3caa7>\u001b[0m in \u001b[0;36mdocument_freq\u001b[1;34m(data, common_words, dictionary)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mword_freq\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# add the idf value to each sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'ItemsView' object does not support indexing"
     ]
    }
   ],
   "source": [
    "# tf-idf function\n",
    "def document_freq(data, common_words, dictionary):\n",
    "    \n",
    "    # initialize rows\n",
    "    rows_list = []\n",
    "    \n",
    "    \n",
    "    # count # of sentences the word appears in and overwrite the actual value in dictionary\n",
    "    for i, word in enumerate(common_words):\n",
    "        word_freq = 0\n",
    "        for doc in data:\n",
    "            for sentence in doc.sents:\n",
    "                if word in str(sentence):\n",
    "                    word_freq += 1\n",
    "        dictionary.items()[i][1] = word_freq\n",
    "        \n",
    "    # add the idf value to each sentence\n",
    "    for j, sentence in enumerate(all_sents):\n",
    "        for i, word in enumerate(common_words):       \n",
    "            # find idf\n",
    "            sentence_list = []\n",
    "            if (word in sentence and dictionary.items()[i][1] != 0):\n",
    "                sentence_list.append(0)\n",
    "            else:\n",
    "                sentence_list.append(np.log2(len(all_sents)/dictionary.items()[i][1]))\n",
    "                \n",
    "        rows_list.append(list(sentence_list), sentence_names[j])\n",
    "       \n",
    "    return pd.DataFrame(rows_list, columns=(common_words + ['source']))\n",
    "\n",
    "start_time = time.time()\n",
    "tf_df = document_freq(nlp_docs, columns, dic)\n",
    "print(\"--- Fitted in %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = df.iloc[:, 5384]\n",
    "X = np.array(df.iloc[:, :5384])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=66)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = ensemble.GradientBoostingClassifier()\n",
    "# train = clf.fit(X_train, y_train)\n",
    "\n",
    "# print('Training set score:', clf.score(X_train, y_train))\n",
    "# print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "\n",
    "# store tf-idf\n",
    "sims = gensim.similarities.Similarity('/usr/workdir/', tf_idf[corpus], num_features=len(dic))\n",
    "print(sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
