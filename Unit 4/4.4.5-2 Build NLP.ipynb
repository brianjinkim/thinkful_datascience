{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import gutenberg, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    # Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    \n",
    "    # Get rid of headings in square brackets.\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    # Get rid of chapter titles.\n",
    "    text = re.sub(r'Chapter \\d+','',text)\n",
    "    text = re.sub(r'CHAPTER \\d+', '', text)\n",
    "    text = re.sub(\"\\\\n\\\\n.*?\\\\n\\\\n\", '', text)\n",
    "  \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "ball = gutenberg.raw('chesterton-ball.txt')\n",
    "brown = gutenberg.raw('chesterton-brown.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "\n",
    "# clean texts\n",
    "ball = text_cleaner(ball)\n",
    "brown = text_cleaner(brown)\n",
    "thursday = text_cleaner(thursday)\n",
    "\n",
    "# run spacy and analyze the documents\n",
    "nlp = spacy.load('en')\n",
    "ball_doc = nlp(ball)\n",
    "brown_doc = nlp(brown)\n",
    "thursday_doc = nlp(thursday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put sentences into a list\n",
    "brown_sents = []\n",
    "ball_sents = []\n",
    "thursday_sents = []\n",
    "for sentence in brown_doc.sents:\n",
    "    brown_sents.append(sentence)\n",
    "for sentence in ball_doc.sents:\n",
    "    ball_sents.append(sentence)\n",
    "for sentence in thursday_doc.sents:\n",
    "    thursday_sents.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sentences\n",
      "----------------------------------------\n",
      "Brown sentences: 3534\n",
      "Ball sentences: 4272\n",
      "Thursday sentences: 3053\n"
     ]
    }
   ],
   "source": [
    "print('Length of sentences\\n'+'-'*40+'\\nBrown sentences: {}\\nBall sentences: {}\\nThursday sentences: {}'.format(len(brown_sents), len(ball_sents), len(thursday_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown_sents = brown_sents[:2000]\n",
    "# ball_sents = ball_sents[:2000]\n",
    "# thursday_sents = thursday_sents[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10859 sentences and 99833 tokens\n"
     ]
    }
   ],
   "source": [
    "# combine the sentences together\n",
    "def appendSentences(doc):\n",
    "    sentence_list = []\n",
    "    for sentence in doc:\n",
    "        sentence = [\n",
    "            token.lemma_.lower()\n",
    "            for token in sentence\n",
    "            if not token.is_stop\n",
    "            and not token.is_punct\n",
    "        ]\n",
    "        sentence_list.append(sentence)\n",
    "    return sentence_list \n",
    "\n",
    "# get count of tokens\n",
    "def getTokenCount(doc):\n",
    "    count = 0\n",
    "    for sentence in doc:\n",
    "        for token in sentence:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "all_sents = appendSentences(brown_sents) + appendSentences(ball_sents) + appendSentences(thursday_sents)\n",
    "token_count = getTokenCount(all_sents)\n",
    "\n",
    "print('We have {} sentences and {} tokens'.format(len(all_sents), token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\brian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model_cbow = word2vec.Word2Vec(\n",
    "    all_sents,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('business', 0.9922202229499817), ('fancy', 0.9893401861190796), ('vulgar', 0.9888213872909546), ('desperate', 0.9886668920516968), ('dubosc', 0.9886517524719238), ('buy', 0.9883180260658264), ('death', 0.9876230955123901), ('anarchist', 0.987596869468689), ('social', 0.9872456789016724), ('train', 0.9869893789291382)]\n",
      "breakfast\n"
     ]
    }
   ],
   "source": [
    "# List of words in model.\n",
    "vocab = model_cbow.wv.vocab.keys()\n",
    "\n",
    "print(model_cbow.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "\n",
    "# Similarity is calculated using the cosine, so again 1 is total\n",
    "# similarity and 0 is no similarity.\n",
    "\n",
    "# One of these things is not like the other...\n",
    "print(model_cbow.wv.doesnt_match(\"breakfast marriage dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg = word2vec.Word2Vec(\n",
    "    all_sents,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=1,          # skip-gram.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('madness', 0.8698895573616028), ('keeper', 0.8633471131324768), ('librarian', 0.8612260222434998), ('though', 0.8536661863327026), ('coincidence', 0.8517497181892395), ('rope', 0.851563572883606), ('stiff', 0.8512357473373413), ('deadly', 0.8511946797370911), ('memory', 0.8507880568504333), ('pure', 0.84935063123703)]\n",
      "breakfast\n"
     ]
    }
   ],
   "source": [
    "# List of words in model.\n",
    "vocab = model_sg.wv.vocab.keys()\n",
    "\n",
    "print(model_sg.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "\n",
    "# Similarity is calculated using the cosine, so again 1 is total\n",
    "# similarity and 0 is no similarity.\n",
    "\n",
    "# One of these things is not like the other...\n",
    "print(model_sg.wv.doesnt_match(\"breakfast marriage dinner lunch\".split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
